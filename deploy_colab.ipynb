{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSvjqzeTiPIg","executionInfo":{"status":"ok","timestamp":1752607457213,"user_tz":-300,"elapsed":74876,"user":{"displayName":"MUHAMMAD FAHAD IRSHAD","userId":"02672423148254639969"}},"outputId":"d64b5691-1452-446d-9077-42fe47aad5f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h‚úÖ All packages installed successfully!\n"]}],"source":["# Install required packages\n","%pip install gradio==5.37.0 -q\n","%pip install optuna -q\n","%pip install plotly -q\n","%pip install xgboost -q\n","%pip install lightgbm -q\n","%pip install scikit-learn -q\n","%pip install pandas -q\n","%pip install numpy -q\n","%pip install scipy -q\n","\n","print(\"‚úÖ All packages installed successfully!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7P3mUvWZiPIi"},"outputs":[],"source":["# Import all required libraries\n","import gradio as gr\n","import pandas as pd\n","import numpy as np\n","import optuna\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import asyncio\n","import logging\n","from datetime import datetime, timedelta\n","from typing import Dict, List, Any, Optional, Tuple, Callable\n","from dataclasses import dataclass, asdict\n","from abc import ABC, abstractmethod\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Machine Learning imports\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.linear_model import Ridge, ElasticNet\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","from sklearn.pipeline import Pipeline\n","from scipy import stats\n","import xgboost as xgb\n","from xgboost import XGBRegressor\n","import lightgbm as lgb\n","from lightgbm import LGBMRegressor\n","\n","print(\"‚úÖ All imports successful!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QowZ4pY9iPIi"},"outputs":[],"source":["# ==========================================\n","# AUTONOMOUS AI SYSTEM FOR FLOOD PREDICTION\n","# ==========================================\n","\n","# Configuration and Data Classes\n","@dataclass\n","class AgentConfig:\n","    name: str\n","    enabled: bool = True\n","    update_frequency: int = 3600\n","    performance_threshold: float = 0.85\n","    max_retries: int = 3\n","    timeout: int = 300\n","\n","@dataclass\n","class ModelPerformance:\n","    model_name: str\n","    rmse: float\n","    r2: float\n","    mae: float\n","    timestamp: datetime\n","    cross_val_score: float\n","    feature_count: int\n","\n","@dataclass\n","class DataQualityReport:\n","    timestamp: datetime\n","    missing_values: Dict[str, float]\n","    data_drift_score: float\n","    feature_correlations: Dict[str, float]\n","    outlier_percentage: float\n","    distribution_changes: Dict[str, float]\n","\n","class BaseAgent(ABC):\n","    def __init__(self, config: AgentConfig):\n","        self.config = config\n","        self.logger = logging.getLogger(f\"Agent.{config.name}\")\n","        self.is_running = False\n","        self.last_execution = None\n","        self.performance_history = []\n","\n","    @abstractmethod\n","    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:\n","        pass\n","\n","class DataMonitoringAgent(BaseAgent):\n","    def __init__(self, config: AgentConfig):\n","        super().__init__(config)\n","        self.baseline_stats = None\n","        self.drift_threshold = 0.3\n","\n","    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:\n","        try:\n","            current_data = context.get('current_data')\n","            if current_data is None:\n","                return {'status': 'no_data', 'action_required': False}\n","\n","            quality_report = self._assess_data_quality(current_data)\n","            drift_detected = self._detect_data_drift(current_data)\n","\n","            return {\n","                'status': 'completed',\n","                'quality_report': asdict(quality_report),\n","                'drift_detected': drift_detected,\n","                'action_required': drift_detected or quality_report.missing_values.get('total', 0) > 0.1\n","            }\n","        except Exception as e:\n","            self.logger.error(f\"Data monitoring failed: {str(e)}\")\n","            return {'status': 'error', 'error': str(e), 'action_required': True}\n","\n","    def _assess_data_quality(self, data: pd.DataFrame) -> DataQualityReport:\n","        missing_values = {col: data[col].isnull().sum() / len(data) for col in data.columns}\n","        missing_values['total'] = data.isnull().sum().sum() / (len(data) * len(data.columns))\n","\n","        # Calculate outliers using IQR method\n","        outlier_count = 0\n","        for col in data.select_dtypes(include=[np.number]).columns:\n","            Q1 = data[col].quantile(0.25)\n","            Q3 = data[col].quantile(0.75)\n","            IQR = Q3 - Q1\n","            outliers = ((data[col] < (Q1 - 1.5 * IQR)) | (data[col] > (Q3 + 1.5 * IQR))).sum()\n","            outlier_count += outliers\n","\n","        outlier_percentage = outlier_count / (len(data) * len(data.select_dtypes(include=[np.number]).columns))\n","\n","        # Feature correlations\n","        numeric_data = data.select_dtypes(include=[np.number])\n","        correlations = {}\n","        if len(numeric_data.columns) > 1:\n","            corr_matrix = numeric_data.corr()\n","            correlations = {f\"{col1}_{col2}\": corr_matrix.loc[col1, col2]\n","                           for col1 in corr_matrix.columns\n","                           for col2 in corr_matrix.columns if col1 != col2}\n","\n","        return DataQualityReport(\n","            timestamp=datetime.now(),\n","            missing_values=missing_values,\n","            data_drift_score=0.0,  # Will be calculated in drift detection\n","            feature_correlations=correlations,\n","            outlier_percentage=outlier_percentage,\n","            distribution_changes={}\n","        )\n","\n","    def _detect_data_drift(self, current_data: pd.DataFrame) -> bool:\n","        if self.baseline_stats is None:\n","            self.baseline_stats = self._compute_baseline_stats(current_data)\n","            return False\n","\n","        drift_detected = False\n","        for col in current_data.select_dtypes(include=[np.number]).columns:\n","            if col in self.baseline_stats:\n","                try:\n","                    # Kolmogorov-Smirnov test\n","                    ks_stat, p_value = stats.ks_2samp(\n","                        self.baseline_stats[col]['sample'],\n","                        current_data[col].dropna()\n","                    )\n","                    if p_value < 0.05:  # Significant drift detected\n","                        drift_detected = True\n","                        self.logger.warning(f\"Data drift detected in column {col} (p-value: {p_value:.4f})\")\n","                except Exception as e:\n","                    self.logger.error(f\"Error detecting drift for {col}: {str(e)}\")\n","\n","        return drift_detected\n","\n","    def _compute_baseline_stats(self, data: pd.DataFrame) -> Dict[str, Any]:\n","        stats_dict = {}\n","        for col in data.select_dtypes(include=[np.number]).columns:\n","            stats_dict[col] = {\n","                'mean': data[col].mean(),\n","                'std': data[col].std(),\n","                'min': data[col].min(),\n","                'max': data[col].max(),\n","                'sample': data[col].dropna().sample(min(1000, len(data))).values\n","            }\n","        return stats_dict\n","\n","print(\"‚úÖ Data Monitoring Agent defined\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YD4TiH8iPIj"},"outputs":[],"source":["# SIMPLE COLAB DEPLOYMENT\n","# Just run this cell and upload your agentic_ai_system.py file when prompted\n","\n","import os\n","from google.colab import files\n","\n","# Upload the agentic_ai_system.py file\n","print(\"üìÅ Please upload your agentic_ai_system.py file:\")\n","uploaded = files.upload()\n","\n","# Save the uploaded file\n","for filename in uploaded.keys():\n","    with open(filename, 'wb') as f:\n","        f.write(uploaded[filename])\n","    print(f\"‚úÖ Saved {filename}\")\n","\n","# Now import and run the system\n","exec(open('agentic_ai_system.py').read())\n","\n","print(\"üöÄ System ready! Now upload and run app.py or create your own interface.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOMjtCjIiPIk"},"outputs":[],"source":["# LAUNCH THE AUTONOMOUS AI SYSTEM\n","# Run this after uploading your files\n","\n","# Option 1: Run the full Gradio app\n","%run app.py\n","\n","# Option 2: Or create a simple demo here\n","def create_simple_demo():\n","    \"\"\"Create a simple demo of the autonomous system\"\"\"\n","\n","    # Generate sample flood prediction data\n","    np.random.seed(42)\n","    n_samples = 1000\n","\n","    sample_data = pd.DataFrame({\n","        'MonsoonIntensity': np.random.normal(5, 2, n_samples),\n","        'TopographyDrainage': np.random.normal(3, 1, n_samples),\n","        'RiverManagement': np.random.normal(2, 0.5, n_samples),\n","        'Deforestation': np.random.normal(1, 0.3, n_samples),\n","        'Urbanization': np.random.normal(2, 0.8, n_samples),\n","        'ClimateChange': np.random.normal(3, 1, n_samples),\n","        'DamsQuality': np.random.normal(4, 1.2, n_samples),\n","        'Siltation': np.random.normal(2, 0.6, n_samples),\n","        'AgriculturalPractices': np.random.normal(3, 0.9, n_samples),\n","        'Encroachments': np.random.normal(1, 0.4, n_samples),\n","        'IneffectiveDisasterPreparedness': np.random.normal(2, 0.7, n_samples),\n","        'DrainageSystems': np.random.normal(3, 1, n_samples),\n","        'CoastalVulnerability': np.random.normal(2, 0.8, n_samples),\n","        'Landslides': np.random.normal(1, 0.5, n_samples),\n","        'Watersheds': np.random.normal(4, 1.5, n_samples),\n","        'DeterioratingInfrastructure': np.random.normal(2, 0.6, n_samples),\n","        'PopulationScore': np.random.normal(3, 1, n_samples),\n","        'WetlandLoss': np.random.normal(2, 0.7, n_samples),\n","        'InadequatePlanning': np.random.normal(2, 0.8, n_samples),\n","        'PoliticalFactors': np.random.normal(1, 0.3, n_samples)\n","    })\n","\n","    # Create target variable (FloodProbability)\n","    sample_data['FloodProbability'] = (\n","        0.3 * sample_data['MonsoonIntensity'] +\n","        0.2 * sample_data['TopographyDrainage'] +\n","        0.1 * sample_data['ClimateChange'] +\n","        0.1 * sample_data['Deforestation'] +\n","        0.1 * sample_data['Urbanization'] +\n","        0.2 * sample_data['IneffectiveDisasterPreparedness'] +\n","        np.random.normal(0, 0.5, n_samples)\n","    )\n","\n","    # Clip to [0, 1] range\n","    sample_data['FloodProbability'] = np.clip(sample_data['FloodProbability'], 0, 1)\n","\n","    return sample_data\n","\n","# Create demo data\n","demo_data = create_simple_demo()\n","print(f\"‚úÖ Created demo dataset with {len(demo_data)} samples\")\n","print(f\"üìä Features: {list(demo_data.columns[:-1])}\")\n","print(f\"üéØ Target: FloodProbability (range: {demo_data['FloodProbability'].min():.3f} - {demo_data['FloodProbability'].max():.3f})\")\n","\n","print(\"\\\\nüöÄ Ready to run autonomous system!\")\n","print(\"\\\\nüìñ Instructions:\")\n","print(\"1. Upload agentic_ai_system.py and app.py files\")\n","print(\"2. Run '%run app.py' to launch the full interface\")\n","print(\"3. Or manually run the orchestrator with the demo data\")\n"]},{"cell_type":"raw","metadata":{"vscode":{"languageId":"raw"},"id":"zZ5V9-RmiPIk"},"source":["# ü§ñ Agentic AI Flood Prediction System - Google Colab Deployment\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-username/your-repo/blob/main/deploy_colab.ipynb)\n","\n","This notebook deploys your autonomous flood prediction system on Google Colab.\n","\n","## üéØ What This Does:\n","- Sets up the complete agentic AI system\n","- Runs autonomous flood prediction cycles\n","- Provides interactive dashboard\n","- Handles model training and deployment automatically\n","\n","## üöÄ Benefits:\n","- **Free to use** with Google Colab\n","- **GPU acceleration** available\n","- **Easy sharing** via links\n","- **No local setup** required\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}